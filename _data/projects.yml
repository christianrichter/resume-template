# Projects
# uncomment the urls if you wish to display them, and add your own URL

# rtlplus
- project: Cloud based data warehouse
  industry: Entertainment
  role: Solution Architect / Developer
  duration: Jul, 2020 &mdash; Present
  technologies: AWS Cloud, Kubernetes, Docker, Spark, Airflow, Kafka, Terraform, Python, Kustomize, GitLab
  # url: "#"
  description: >
    Design and implementation of a cloud-based data ware house for processing user related data.
    <ul class="resume-item-list">
    <li>Implemented workflows to fetch data from various third-party providers</li>
    <li>Building and enabling a team to create new ETL processes</li>
    <li>Realtime integration of a market automation software suite</li>
    <li>Implemented modern ETL processing environment using Airflow, Spark and Kubernetes</li>
    <li>General advice on data architecture and data management</li>
    </ul>

# swarovski
- project: Sensor data processing
  industry: Consumer Goods
  role: Solution Architect
  duration: Aug, 2019 &mdash; Dec, 2019
  # url: "#"
  technologies: AWS Cloud, Kubernetes, Kafka, Spark, Bamboo, Java, Docker, Terraform
  description: >
    Design of AWS managed infrastructure platform for sensor data processing, extension of an existing data science environment.
    <ul class="resume-item-list">
    <li>Advice on design and tools for building a Kubernetes based infrastructure platform for sensor data processing</li>
    <li>Design and implementation of infrastructure components on Kubernetes</li>
    <li>Design and implementation of the ETL pipeline for data collection</li>
    <li>Design and implementation of CI/CD pipeline with Bamboo & Kubernetes</li>
    </ul>

# volkswagen
- project: Data analytics on car measurement data
  industry: Automotive
  role: Solution Architect / Developer
  duration: Jan, 2019 &mdash; Mar, 2020
  # url: "#"
  technologies: AWS Cloud, Lambda, IAM, Airflow, Kubernetes, Terraform, Python, Jenkins
  description: >
    Design and implementation of a cloud-based data warehouse for evaluation of vehicle data.
    Design and implementation of a data science environment.
    <ul class="resume-item-list">
    <li>Extented a prototype and put into operational readiness for a production environment</li>
    <li>Design and implementation of a CI / CD pipeline</li>
    <li>Setup of project structure and release management</li>
    <li>Implemented various ETL pipelines for car measurement data collection, validation and transformation</li>
    </ul>


# template
- project: Data science environment
  industry: Consumer Goods
  role: Solution Architect / Developer
  duration: Oct, 2017 &mdash; Nov, 2018
  # url: "#"
  technologies: AWS Cloud, Kubernetes, Spark, R, NiFi, Terraform, Docker, Jupyter NB
  description: >
    Design and implementation of a cloud-based data warehouse & data science environment.
    <ul class="resume-item-list">
    <li>Designing data warehouse architecture & data storage strategies</li>
    <li>Architecture proposal of a dynamically scalable data warehouse</li>
    <li>Implementation of infrastructure components in Terraform & Kubernetes</li>
    <li>Implementation of infrastructure components in Kubernetes</li>
    <li>Development of ETL pipelines for data collection</li>
    </ul>


# aixigo
- project: Micro service architecture
  industry: Financial services
  role: System Architect
  duration: May, 2017 &mdash; Dec, 2018
  # url: "#"
  technologies: Micro Services, Java, Docker, Kafka, Liquibase, Jekins
  description: >
    Support conception and implementation/migration of a monolith into a micro service architecture.
    <ul class="resume-item-list">
    <li>Alignment and coordination of different teams regarding technology usage</li>
    <li>Introduction of Kafka as the central message bus for micro service communications</li>
    <li>Introduction of LiquiBase for database schema management</li>
    <li>Professional / technical support for a specific micro service</li>
    </ul>


# open grid
- project: Big Data vendor evaluation
  industry: Energy
  role:  Requirements Engineer/ Solution Architect
  duration: Mar, 2017
  # url: "#"
  technologies: Hortonworks, Cloudera, SAP Cloud, Apache NiFi, AWS Cloud, MS Azure, Terraform
  description: >
    Support in evaluating big data providers
    <ul class="resume-item-list">
    <li>Acquisition and documentation of the technical requirements for setting up and operating an Apache Hadoop based data warehouse</li>
    <li>Obtaining offers from various providers, preparing information for decision-making</li>
    <li>Implementation of a prototype for data collection</li>
    </ul>


# gfk
- project: Big Data warehouse
  industry: Market research
  role: Solution Architect / Developer
  duration: Jan, 2017 &mdash; Aug, 2017
  # url: "#"
  technologies: Spark, SparkR, Hadoop, Hive, Jupyter, AWS Cloud, R, Bamboo, Terraform
  description: >
    Design and implementation of a cloud-based big data warehouse in the AWS Cloud for market research analytics.
    <ul class="resume-item-list">
    <li>Technical project management</li>
    <li>Design of an architecture based on AWS cloud infrastructure and managed services</li>
    <li>Implementation of ETL data pipelines</li>
    <li>Development of data warehouse / workflow management</li>
    <li>Data preparation / process management</>
    </ul>

# basel
- project: Workshop Big Data technologies
  industry: Education
  role: Data Engineer
  duration: Oct, 2016
  # url: "#"
  technologies: Hadoop, Spark, AWS Cloud, MapReduce, Hive, Pig, R, Terraform
  description: >
    Workshop Big Data Technologies - Introduction and Getting Started.
    <ul class="resume-item-list">
    <li>Conducting a 3-day workshop</li>
    <li>Introduction to Big Data / Hadoop ecosystem</li>
    <li>Practical exercise using big data tools in the AWS Cloud</li>
    </ul>


# helix leisure
- project: Architecture Review
  industry: Entertainment
  role: Data Engineer / Solution Architect
  duration: Jun, 2016 &mdash; Dec, 2016
  # url: "#"
  technologies: Hadoop, Spark, AWS Cloud, Scala, MapReduce, JCascalog, RedShift
  description: >
    Architecture review and design and implementation of a realtime aggregator for machine statistics.
    <ul class="resume-item-list">
    <li>Review and assessment of the existing architecture and data model design</li>
    <li>Implementation workshop data management/Lambda architecture</li>
    <li>Design and implementation of a realtime layer with Spark Streaming</li>
    </ul>

# otto
- project: ETL Pipelines
  industry: Online retail
  role: Solution Architect, Developer
  duration: Feb, 2016 &mdash; Jul, 2016
  # url: "#"
  technologies: Hadoop, Hive, Spark, Redis, Kafka, Avro, Scala, HCatalog, Schedoscope
  description: >
    Support in the development of ETL processes on a Hadoop based DWH.
    <ul class="resume-item-list">
    <li>Planning and implementation of a hive export module</li>
    <li>Implementation of a Kafka & Redis export module as part of an open source project</li>
    <li>Implementation of an analysis algorithm for click stream analytics</li>
    </ul>


# gfk
- project: CI/CD Pipelines
  industry: Market research
  role: Solution Architect, Developer
  duration: Dec, 2015 &mdash; Aug, 2016
  # url: "#"
  technologies: AWS Cloud, Hadoop, Spark, Bamboo, Git, Terraform, Vagrant, InfluxDB
  description: >
    Design and implementation of a continuous deployment & delivery pipeline for data-driven applications in cloud environments.
    <ul class="resume-item-list">
    <li>Design and implementation of a big data infrastructure in the AWS Cloud</li>
    <li>Design and implementation of a continuous deployment pipeline</li>
    <li>Technical management of an customer internal team</li>
    </ul>

# radioopt
- project: Data warehouse
  industry: Technology
  role: Solution Architect
  duration: Jul, 2015 &mdash; Oct, 2015
  # url: "#"
  technologies: Hadoop, Impala, Hive, ETL, AWS Cloud
  description: >
    Conception and implementation of a data ware house based on big data technologies - OLAP workload.
    <ul class="resume-item-list">
    <li>Planning and implementation of the cluster infrastructure</li>
    <li>Evaluation of various input formats with regard to performance</li>
    <li>Preparation, execution and documentation of load tests</li>
    </ul>

# technicolor
- project: Data analytics on wifi device data
  industry: Consumer Goods
  role: Solution Architect / Developer
  duration: Jul, 2014 &mdash; Jun, 2015
  # url: "#"
  technologies: Hadoop, Samza, Spark, Kafka, Java, ETL, AWS
  description: >
    Design and implementation of a big data system for batch and real-time data processing of machine generated data.
    <ul class="resume-item-list">
    <li>Planning and implementation of the deployment environment</li>
    <li>Evaluation of various technologies for data acquisition / data processing</li>
    <li>Implementation of a distributed, fail-safe high throughput messaging and analysis system for machine data (Lambda Architecture)</li>
    <li>Technical management of a team</li>
    </ul>

# ubisoft
- project: Game Analytics
  industry: Entertainment
  role: Solution Architect / Developer
  duration: Mar, 2013 &mdash; Sep, 2014
  # url: "#"
  technologies: Hadoop, Map / Reduce, Kafka, Hive, ETL, Java, Linux
  description: >
    Design and implementation of Hadoop based data warehouse for online game analytics.
    <ul class="resume-item-list">
    <li>Planning and implementation of a data warehouse</li>
    <li>Evaluation of different approaches for data collection</li>
    <li>Selection of suitable technologies</li>
    <li>Technical management / coordination of a distributed team (GER, CN, CAN)</li>
    <li>Implementation of a distributed, fail-safe high throughput messaging system</li>
    </ul>


# telekom
- project: Hadoop on Demand
  industry: Telecommunication
  role: Solution Architect / Developer
  duration: Feb, 2013 &mdash; Jun, 2014
  # url: "#"
  technologies: Hadoop, OpenStack, Opscode Chef, Java, Linux
  description: >
    Design and implementation of a big data infrastructure in virtualized environments.
    <ul class="resume-item-list">
    <li>Planning and implementation of a big data deployment infrastructure</li>
    <li>Implementation deployment process for Hadoop Cluster on demand in a virtualized environment</li>
    <li>Prototype implementation of various algorithms with the map/reduce framework</li>
    </ul>

# gfk
- project: Data analytics on geo location data
  industry: Market research
  role: Solution Architect / Developer
  duration: Nov, 2012 &mdash; Aug, 2015
  # url: "#"
  technologies: Apache Hadoop, Hive, Flume, Java, Spring, Puppet, Ubuntu Linux, AWS
  description: >
    Design and implementation of a big data architecture for evaluating telecommunications data.
    <ul class="resume-item-list">
    <li>Planning and implementation of the network setup</li>
    <li>Planning and implementation of a medium sized Hadoop cluster</li>
    <li>Set up deployment process, including monitoring</li>
    <li>Implementation of a data integration framework for high volume data storage</li>
    </ul>
